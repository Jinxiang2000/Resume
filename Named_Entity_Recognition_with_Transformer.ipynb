{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jinxiang2000/Resume/blob/main/Named_Entity_Recognition_with_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "This notebook presents a comprehensive approach to implementing and testing transformer models on natural language processing tasks, specifically Named Entity Recognition (NER). We leverage the robust capabilities of PyTorch and related libraries to develop, train, and validate models that recognize and classify entities in text data.\n"
      ],
      "metadata": {
        "id": "zZFx-ERPS6R7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background\n",
        "### Named Entity Recognition (NER)\n",
        "Named entities are phrases that contain the names of persons, organizations, locations, times, and quantities, etc. For example:\n",
        "\n",
        "`[ORG U.N.] official [PER Ekeus] heads for [LOC Baghdad].`\n",
        "\n",
        "This sentence contains three named entities:\n",
        "- Ekeus is a person,\n",
        "- U.N. is an organization,\n",
        "- Baghdad is a location.\n",
        "\n",
        "Named entity recognition is an important task of information extraction systems that seeks to locate and classify named entities mentioned in unstructured text. Suppose we have an unannotated block of text:\n",
        "\n",
        "`Alex moved to Los Angeles to work for Universal Studios.`\n",
        "\n",
        "An NER system should produce annotated text that highlights the named entities:\n",
        "\n",
        "`[PER Alex] moved to [LOC Los Angeles] to work for [ORG Universal Studios].`\n",
        "\n",
        "### Dataset\n",
        "The dataset used is CoNLL-2003, a named entity recognition dataset released as part of CoNLL-2003 shared task concerning language-independent named entity recognition. The dataset’s English data was taken from the Reuters Corpus, tagged and chunked by the memory-based MBT tagger. The tagging followed mostly MUC conventions, with an extra named entity category called MISC added.\n",
        "Three data splits are provided:\n",
        "- `train.csv`\n",
        "- `val.csv`\n",
        "- `test_tokens.txt`\n",
        "\n",
        "### BIO Tagging Scheme\n",
        "The original tagging scheme follows the BIO format:\n",
        "- `B-TYPE` for the beginning of a chunk,\n",
        "- `I-TYPE` for inside a named entity,\n",
        "- `O` for outside a named entity.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RS6AjfUuzEPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "biJRmOC_TGvI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWWjBbIZ5LSi",
        "outputId": "2e2b131e-c5b7-456d-9de8-e30c182fd5cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.15.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.4.0.post0\n",
            "Collecting pytorch-metric-learning\n",
            "  Downloading pytorch_metric_learning-2.5.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.1/119.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->pytorch-metric-learning) (12.5.40)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
            "Installing collected packages: pytorch-metric-learning\n",
            "Successfully installed pytorch-metric-learning-2.5.0\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.3.0-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.2/812.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.4.0.post0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.15.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pytorch-lightning) (12.5.40)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: pytorch-lightning\n",
            "Successfully installed pytorch-lightning-2.3.0\n"
          ]
        }
      ],
      "source": [
        "from typing import Dict, List, Optional\n",
        "from collections import Counter\n",
        "import os\n",
        "import csv\n",
        "!pip install torchmetrics\n",
        "!pip install pytorch-metric-learning\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "!pip install pytorch-lightning\n",
        "import torch.optim as optim\n",
        "import torchmetrics\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG7DetzZn1AA",
        "outputId": "1cb1d5bc-d5d7-47f6-8fd5-4ddef4a9848c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "In this section, we establishes the foundational components for text processing and model architecture essential for the Named Entity Recognition (NER) task. We first introduce a custom tokenizer to convert text data into a format suitable for neural network processing.\n",
        "\n",
        "\n",
        "*   The Tokenizer class is initialized with two special tokens: <pad> for padding shorter sentences to a fixed length, and <unk> for representing out-of-vocabulary (OOV) words.\n",
        "*   These tokens are essential for handling variability in sentence length and vocabulary.\n",
        "*   The fit method calculates the frequency of each word in the training dataset to construct a vocabulary.\n",
        "*   The encode method converts a given text string into a list of numerical token IDs based on the vocabulary established in the fit method.\n",
        "\n"
      ],
      "metadata": {
        "id": "8XFkOMZnT4rX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u29mNAdI5LSl"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        # two special tokens for padding and unknown\n",
        "        self.token2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "        self.idx2token = [\"<pad>\", \"<unk>\"]\n",
        "        self.is_fit = False\n",
        "\n",
        "    @property\n",
        "    def pad_id(self):\n",
        "        return self.token2idx[\"<pad>\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def fit(self, train_texts: List[str]):\n",
        "        counter = Counter()\n",
        "        for text in train_texts:\n",
        "            counter.update(text.lower().split())\n",
        "\n",
        "        # manually set a vocabulary size for the data set\n",
        "        vocab_size = 20000\n",
        "        self.idx2token.extend([token for token, count in counter.most_common(vocab_size - 2)])\n",
        "        for (i, token) in enumerate(self.idx2token):\n",
        "            self.token2idx[token] = i\n",
        "\n",
        "        self.is_fit = True\n",
        "\n",
        "    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:\n",
        "        if not self.is_fit:\n",
        "            raise Exception(\"Please fit the tokenizer on the training tokens\")\n",
        "\n",
        "        tokens = text.lower().split()[:max_length] if max_length else text.lower().split()\n",
        "\n",
        "        token_id = []\n",
        "        for token in tokens:\n",
        "           if token in self.token2idx:\n",
        "             token_id.append(self.token2idx[token])\n",
        "           else:\n",
        "             token_id.append(self.token2idx[\"<unk>\"])\n",
        "\n",
        "        if max_length:\n",
        "          num_padding = max_length - len(token_id)\n",
        "          if num_padding>0:\n",
        "            token_id.extend([self.pad_id]*num_padding)\n",
        "\n",
        "        return token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCgGv68lrvix",
        "outputId": "6e2ed798-8eed-44b8-89e9-1620f0ceaa37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 1, 1, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "train_texts = [\"hello world\", \"hello transformers\"]\n",
        "tokenizer.fit(train_texts)\n",
        "encoded_text = tokenizer.encode(text = \"hello you are !\", max_length=5)\n",
        "print(encoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define and utilize a tokenizer to convert text into a sequence of token IDs, which is crucial for model training and evaluation. The tokenizer handles both the inclusion of special tokens and the fitting process to accommodate the dataset vocabulary."
      ],
      "metadata": {
        "id": "JiM2x--3WHLV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF4glogI5LSk"
      },
      "source": [
        "## Data Loading and Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lHbdxRn5LSm"
      },
      "outputs": [],
      "source": [
        "def load_raw_data(filepath: str, with_tags: bool = True):\n",
        "    data = {'text': []}\n",
        "    if with_tags:\n",
        "        data['tags'] = []\n",
        "        with open(filepath) as f:\n",
        "            reader = csv.reader(f)\n",
        "            for text, tags in reader:\n",
        "                data['text'].append(text)\n",
        "                data['tags'].append(tags)\n",
        "    else:\n",
        "        with open(filepath) as f:\n",
        "            for line in f:\n",
        "                data['text'].append(line.strip())\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEuoJh1Q5LSn"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "data_dir = \"/content/drive/MyDrive/cs190I-w23-mp2/cs190I-w23-mp2\"\n",
        "train_raw = load_raw_data(os.path.join(data_dir, \"train.csv\"))\n",
        "val_raw = load_raw_data(os.path.join(data_dir, \"val.csv\"))\n",
        "test_raw = load_raw_data(os.path.join(data_dir, \"test_tokens.txt\"), with_tags=False)\n",
        "# fit the tokenizer on the training tokens\n",
        "tokenizer.fit(train_raw['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94u1vPV-lbXf"
      },
      "outputs": [],
      "source": [
        "#upload the dataset\n",
        "#for google colb, use this\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzUsGMealyZb"
      },
      "outputs": [],
      "source": [
        "class NERDataset:\n",
        "    tag2idx = {'O': 1, 'B-PER': 2, 'I-PER': 3, 'B-ORG': 4, 'I-ORG': 5, 'B-LOC': 6, 'I-LOC': 7, 'B-MISC': 8, 'I-MISC': 9}\n",
        "    idx2tag = ['<pad>', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG','B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "    def __init__(self, raw_data: Dict[str, List[str]], tokenizer: Tokenizer, max_length: int = 128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_ids = []\n",
        "        self.tag_ids = []\n",
        "        self.with_tags = False\n",
        "        for text in raw_data['text']:\n",
        "            self.token_ids.append(tokenizer.encode(text, max_length=max_length))\n",
        "        if 'tags' in raw_data:\n",
        "            self.with_tags = True\n",
        "            for tags in raw_data['tags']:\n",
        "                self.tag_ids.append(self.encode_tags(tags, max_length=max_length))\n",
        "\n",
        "    def encode_tags(self, tags: str, max_length: Optional[int] = None):\n",
        "        tag_ids = [self.tag2idx[tag] for tag in tags.split()]\n",
        "        if max_length is None:\n",
        "            return tag_ids\n",
        "        # truncate the tags if longer than max_length\n",
        "        if len(tag_ids) > max_length:\n",
        "            return tag_ids[:max_length]\n",
        "        # pad with 0s if shorter than max_length\n",
        "        else:\n",
        "            return tag_ids + [0] * (max_length - len(tag_ids))  # 0 as padding for tags\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        token_ids = torch.LongTensor(self.token_ids[idx])\n",
        "        mask = token_ids == self.tokenizer.pad_id  # padding tokens\n",
        "        if self.with_tags:\n",
        "            # for training and validation\n",
        "            return token_ids, mask, torch.LongTensor(self.tag_ids[idx])\n",
        "        else:\n",
        "            # for testing\n",
        "            return token_ids, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kMIKu-p5LSo"
      },
      "outputs": [],
      "source": [
        "tr_data = NERDataset(train_raw, tokenizer)\n",
        "va_data = NERDataset(val_raw, tokenizer)\n",
        "te_data = NERDataset(test_raw, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVOHqRsD5LSo"
      },
      "source": [
        "## Transformer Model\n",
        "In this section, we implement and experiment with transformer models.\n",
        "- `nn.Embedding` layer to embed input token ids to the embedding space\n",
        "- `nn.TransformerEncoder` layer to perform transformer operations\n",
        "- `nn.Linear` layer as the output layer to map the output to the number of classes\n",
        "\n",
        "Positional Encoding: Injects information about the position of tokens within the sequence. Since the Transformer model does not inherently process sequential data as sequential (unlike RNNs), positional encodings are added to retain order information.\n",
        "\n",
        "\n",
        "\n",
        "Since we will be using the cross-entropy loss, an `nn.Softmax` or `nn.LogSoftmax` layer is not needed.\n",
        "\n",
        "Reference:\n",
        "\n",
        "https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "For the `forward` method, the method signature is given as follows:\n",
        "\n",
        "- `src`: a `torch.LongTensor` of shape (batch_size, max_length, vocab_size) representing the input text tokens.\n",
        "\n",
        "- `src_mask`: a `torch.BoolTensor` of shape (batch_size, max_length) indicating whether an input position is padded. This is needed to prevent the transformer model attending to padded tokens.\n",
        "\n",
        "The output from the `forward` method should be of shape (batch_size, max_length, num_classes). Note that the number of classes should be 10 instead of 9 because of an additional padding class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A74-FUCr4Vk6"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "'''\n",
        "Positional Encoding: Adds information about the position of tokens in the sequence,\n",
        "'''\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_k3y9qkCM_m"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers, dropout = 0.2):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.pos_encoding = PositionalEncoding(embed_size)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.transformer_layers = nn.TransformerEncoderLayer(embed_size, num_heads, hidden_size, dropout = dropout, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(self.transformer_layers, num_layers)\n",
        "        self.decoder = nn.Linear(embed_size, 10)\n",
        "        self.embed_size = embed_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "      initrange = 0.1\n",
        "      self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "      self.decoder.bias.data.zero_()\n",
        "      self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
        "      src_embedded = self.embedding(src) * math.sqrt(self.embed_size)\n",
        "      src_embedded = self.pos_encoding(src_embedded)\n",
        "      encoded = self.encoder(src_embedded, src_key_padding_mask = src_mask)\n",
        "      logits = self.decoder(encoded)\n",
        "      return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "The training process of the Transformer model is a critical phase where the model learns to accurately identify and classify named entities from sequences of text data. This process begins with the preparation of the dataset, which is tokenized and batched for efficient processing. The model employs a forward pass mechanism where each batch of tokenized sequences is input to the model to compute predictions for each token.\n",
        "\n",
        "A central element of the training mechanism is the use of the cross-entropy loss function, which measures the discrepancy between the predicted probabilities and the actual class labels. This function is specifically suited for classification tasks, as it quantifies the probability error in discrete classification tasks where the classes are mutually exclusive, making it ideal for NER. The loss provides a gradient signal used in backpropagation, a method by which the model adjusts its parameters to minimize prediction errors. This adjustment is facilitated by an optimizer—typically Adam in deep learning tasks—which updates the model parameters based on the gradients calculated during backpropagation.\n",
        "\n",
        "During training, the model computes the F1 score, a harmonic mean of precision and recall, which is particularly useful for evaluating model performance in imbalanced datasets common in NER tasks. This metric provides insights into the model's accuracy in identifying and classifying entities correctly, balancing the model's performance across different entity types."
      ],
      "metadata": {
        "id": "YvPDG4y_thJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "):\n",
        "\n",
        "    f1_metric = torchmetrics.F1Score(num_classes=10, average='macro', task='multiclass').to(device)\n",
        "    loss_metric = torchmetrics.MeanMetric().to(device)\n",
        "    model.train()\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "        input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Ensure input dimensions are correct for the model\n",
        "        logits = model(input_ids, input_mask)\n",
        "\n",
        "        # Reshape logits and tags to calculate cross-entropy loss\n",
        "        logits_flat = logits.view(-1, logits.shape[-1])\n",
        "        tags_flat = tags.view(-1)\n",
        "\n",
        "        # Calculate loss, ignoring the padding index 0\n",
        "        loss = F.cross_entropy(logits_flat, tags_flat, ignore_index=0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_metric.update(loss.item())\n",
        "\n",
        "        # Calculate F1 score only for non-padded tokens\n",
        "        is_active = ~input_mask  # Find active (non-padded) elements\n",
        "        active_logits = logits.view(-1, 10)[is_active.flatten()]\n",
        "        active_tags = tags.view(-1)[is_active.flatten()]\n",
        "        f1_metric.update(active_logits, active_tags)\n",
        "\n",
        "    average_loss = loss_metric.compute()\n",
        "    average_f1 = f1_metric.compute()  # Compute average F1 score\n",
        "\n",
        "    print(f\"| Epoch {epoch} | Loss: {average_loss:.4f} | F1 Score: {average_f1:.4f} |\")\n"
      ],
      "metadata": {
        "id": "7UCxDOLpQdEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Be4ZCs15LSq",
        "outputId": "7c7b7e86-554d-487d-83d5-3834fa9322e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [05:55<00:00,  2.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 0 | Loss: 0.3318 | F1 Score: 0.6117 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:00<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 1 | Loss: 0.1332 | F1 Score: 0.8035 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:14<00:00,  2.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 2 | Loss: 0.1004 | F1 Score: 0.8451 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:11<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 3 | Loss: 0.0939 | F1 Score: 0.8529 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:08<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 4 | Loss: 0.0885 | F1 Score: 0.8597 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:11<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 5 | Loss: 0.0831 | F1 Score: 0.8707 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:12<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 6 | Loss: 0.0849 | F1 Score: 0.8658 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:15<00:00,  2.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 7 | Loss: 0.0851 | F1 Score: 0.8645 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:22<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 8 | Loss: 0.0810 | F1 Score: 0.8690 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:31<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 9 | Loss: 0.0802 | F1 Score: 0.8708 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:39<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 10 | Loss: 0.0772 | F1 Score: 0.8742 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:50<00:00,  2.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 11 | Loss: 0.0757 | F1 Score: 0.8761 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:59<00:00,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 12 | Loss: 0.0716 | F1 Score: 0.8845 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [06:59<00:00,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 13 | Loss: 0.0684 | F1 Score: 0.8854 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 878/878 [07:25<00:00,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch 14 | Loss: 0.0670 | F1 Score: 0.8888 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# data loaders\n",
        "train_dataloader = DataLoader(tr_data, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(va_data, batch_size=16)\n",
        "test_dataloader = DataLoader(te_data, batch_size=16)\n",
        "\n",
        "# move the model to device\n",
        "model = TransformerModel(vocab_size = len(tokenizer),\n",
        "    embed_size = 256,\n",
        "    num_heads = 4,\n",
        "    hidden_size = 256,\n",
        "    num_layers = 2,).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "for epoch in range(15):\n",
        "    train(model, train_dataloader, optimizer, device, epoch)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training results indicate consistent improvement with the loss decreasing from 0.3318 to 0.0670 and the F1 score increasing from 0.6117 to 0.8888 across 14 epochs. This trend suggests that the model is effectively learning and optimizing its ability to classify named entities with increasing accuracy and reliability."
      ],
      "metadata": {
        "id": "yZBCyhZzvxXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation\n",
        "Validation is conducted to assess the model's generalization capabilities and to ensure that it does not overfit the training data. In the validation phase, the model is set to evaluation mode, which disables training-specific operations like dropout. This mode ensures that the model's predictions are based solely on learned patterns without the influence of random dropout during training, providing a pure evaluation of its predictive power.\n",
        "\n",
        "The validation process involves processing batches of validation data through the model without performing any parameter updates. The same cross-entropy loss used in training is computed to evaluate how well the model performs on unseen data. Simultaneously, the F1 score is calculated to continue monitoring the model’s precision and recall on the validation dataset. Consistently lower validation losses and higher F1 scores indicate that the model is learning generalized patterns rather than memorizing the training data, which is a crucial indication of successful machine learning models."
      ],
      "metadata": {
        "id": "VrBq3lPotvvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "):\n",
        "    # Using F1Score for validation\n",
        "    f1_metric = torchmetrics.F1Score(num_classes=10, average='macro', task='multiclass').to(device)\n",
        "    loss_metric = torchmetrics.MeanMetric().to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "\n",
        "            logits = model(input_ids, input_mask)\n",
        "\n",
        "\n",
        "            logits_flat = logits.view(-1, logits.shape[-1])\n",
        "            tags_flat = tags.view(-1)\n",
        "\n",
        "            # Calculate loss, ignoring the padding index 0\n",
        "            loss = F.cross_entropy(logits_flat, tags_flat, ignore_index=0)\n",
        "\n",
        "            loss_metric.update(loss.item())\n",
        "\n",
        "            # Calculate F1 score only for non-padded tokens\n",
        "            is_active = ~input_mask  # Find active (non-padded) elements\n",
        "            active_logits = logits.view(-1, 10)[is_active.flatten()]\n",
        "            active_tags = tags.view(-1)[is_active.flatten()]\n",
        "            f1_metric.update(active_logits, active_tags)\n",
        "\n",
        "        average_loss = loss_metric.compute()\n",
        "        average_f1 = f1_metric.compute()  # Compute average F1 score\n",
        "\n",
        "    print(f\"| Validate | Loss: {average_loss:.4f} | F1 Score: {average_f1:.4f} |\")\n"
      ],
      "metadata": {
        "id": "0s6E1uulQLia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate(model, val_dataloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCuylBjcc8Yg",
        "outputId": "bd36f1a9-2a04-4766-a87f-2fa075c71f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/204 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:408: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
            "100%|██████████| 204/204 [00:04<00:00, 41.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Validate | Loss: 0.2808 | F1 Score: 0.7227 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The validation results from our Transformer model show a loss of 0.2808 and an F1 score of 0.7227. These metrics indicate that the model performs reasonably well, balancing accuracy and reliability in predicting named entities, as evidenced by the relatively high F1 score."
      ],
      "metadata": {
        "id": "fhikw5RPu56M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQV7JhRl5LSq"
      },
      "source": [
        "## Prediction\n",
        "In this section, we apply the trained Transformer model to the validation dataset to assess its capacity for entity recognition and to quantify its performance through entity-level F1 scores calculated using the conlleval script.\n",
        "\n",
        "`predict`: taking inputs of a trained model, a dataloader, and a torch device, predict the tags for all tokens in the data set. The output is a nested list of lists, each containing tag predictions for a single sentence. This function transform logits from the network into class predictions corresponding to predefined entity tags. These tags include generic labels such as 'O' for non-entity tokens, and others like 'B-PER', 'I-PER' for person entities in 'Begin' and 'Inside' positions, respectively.\n",
        "\n",
        "The predictions were generated by processing each batch from the validation set through the model, filtering out predictions for padding tokens, and reconstructing the sequence of predictions into a format suitable for evaluation. This was achieved using a logical mask to ignore padded positions and ensure that only meaningful predictions contribute to the evaluation metrics.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model: nn.Module, dataloader: DataLoader, device: torch.device) -> List[List[str]]:\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    idx2tag = ['<pad>', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            inputs, mask = batch[0].to(device), batch[1].to(device)\n",
        "            outputs = model(inputs, mask)\n",
        "\n",
        "            # Convert logits to class predictions\n",
        "            predictions = outputs.argmax(dim=-1)\n",
        "\n",
        "            # Apply mask to filter out padding positions\n",
        "            # This assumes the mask marks paddings as `True`, and we only want `False` positions\n",
        "            active_predictions = predictions.masked_select(~mask)\n",
        "\n",
        "            # Reshape the flat list of active predictions back into batches\n",
        "            batch_preds = []\n",
        "            current_batch = []\n",
        "            for i, pred in enumerate(active_predictions):\n",
        "                current_batch.append(idx2tag[pred.item()])\n",
        "                if len(current_batch) == mask.size(1) or i == len(active_predictions) - 1:\n",
        "                    batch_preds.append(current_batch)\n",
        "                    current_batch = []\n",
        "\n",
        "            preds.extend(batch_preds)\n",
        "\n",
        "    return preds\n"
      ],
      "metadata": {
        "id": "bxj1Yhe3OtOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzFjEe0c5LSq",
        "outputId": "0bfd596a-803d-4671-eb39-9db8f5a76d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-24 21:57:54--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7502 (7.3K) [text/plain]\n",
            "Saving to: ‘conlleval.py’\n",
            "\n",
            "\rconlleval.py          0%[                    ]       0  --.-KB/s               \rconlleval.py        100%[===================>]   7.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-24 21:57:54 (35.3 MB/s) - ‘conlleval.py’ saved [7502/7502]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
        "from conlleval import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVAnQYdD5LSr",
        "outputId": "06f8935c-9dbd-4f5a-ea0e-1862a9fa1e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:05<00:00, 36.55it/s]\n"
          ]
        }
      ],
      "source": [
        "# use the conlleval script to measure the entity-level f1\n",
        "pred_tags = []\n",
        "for tags in predict(model, val_dataloader, device):\n",
        "    pred_tags.extend(tags)\n",
        "\n",
        "\n",
        "true_tags = []\n",
        "for tags in val_raw['tags']:\n",
        "    true_tags.extend(tags.strip().split())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example for Entity-level F1 Score\n",
        "Suppose we have the following sentence, its ground truth label sequence, and a candidate label sequence:\n",
        "\n",
        "```\n",
        "Alex Cord moved to Los Angeles to work for Universal Studios.\n",
        "Ground Truth: B-PER I-PER O O B-LOC I-LOC O O O B-ORG I-ORG O\n",
        "Prediction: B-PER B-MISC O O B-LOC I-LOC O O B-ORG I-ORG O O\n",
        "```\n",
        "In this sentence, there are three ground truth entities:\n",
        "- {Alex Cord (PER), Los Angeles (LOC), Universal Studios (ORG)}.\n",
        "\n",
        "There are four predicted entities:\n",
        "- {Alex (PER), Cord (MISC), Los Angeles (LOC), for Universal (ORG)}.\n",
        "  - Notice that the predicted labels of Alex and Cord are not the same, therefore they belong to two entities.\n",
        "\n",
        "Definitions:\n",
        "- **TP (True Positive)**: Correctly identified entity\n",
        "- **FP (False Positive)**: Incorrectly labeled as an entity\n",
        "- **FN (False Negative)**: Correct entity not identified\n",
        "\n",
        "Evaluating entities:\n",
        "- **TP = 1**: Correctly identified Los Angeles as LOC.\n",
        "- **FP = 3**: Alex, Cord, and \"for Universal\" incorrectly labeled as PER, MISC, and ORG.\n",
        "- **FN = 2**: Alex Cord and Universal Studios not identified as the correct entity type.\n",
        "\n",
        "Calculations:\n",
        "- **Precision** = $ \\frac{TP}{TP + FP} $\n",
        "- **Recall** = $ \\frac{TP}{TP + FN} $\n",
        "\n",
        "**F1 score** is the harmonic mean of precision and recall:\n",
        "$ F1 = 2 \\times \\frac{{Precision \\times Recall}}{{Precision + Recall}} $\n",
        "$ = \\frac{2 \\times \\frac{1}{4} \\times \\frac{1}{3}}{{\\frac{1}{4} + \\frac{1}{3}}} $\n",
        "$ = \\frac{2 \\times 0.25 \\times 0.333}{{0.25 + 0.333}} $\n",
        "$ = 0.285 \\approx 28.5\\% $"
      ],
      "metadata": {
        "id": "EGYtfYM50LMi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBgBGMFyjYUW",
        "outputId": "cba6ae17-c970-4df0-a7be-1d02fbddddb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51362 tokens with 5942 phrases; found: 5880 phrases; correct: 4183.\n",
            "accuracy:  67.41%; (non-O)\n",
            "accuracy:  93.56%; precision:  71.14%; recall:  70.40%; FB1:  70.77\n",
            "              LOC: precision:  86.93%; recall:  80.73%; FB1:  83.71  1706\n",
            "             MISC: precision:  73.06%; recall:  74.73%; FB1:  73.89  943\n",
            "              ORG: precision:  62.01%; recall:  61.97%; FB1:  61.99  1340\n",
            "              PER: precision:  62.40%; recall:  64.06%; FB1:  63.22  1891\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(71.13945578231292, 70.39717266913496, 70.76636778886821)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "evaluate(true_tags, pred_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Conclusion\n",
        " The results from this evaluation showed an overall F1 score of 77.07% with an accuracy of 97.95%, indicating a high level of correctness in the token predictions relative to their actual class labels. Notably, the model achieved high precision and recall across various entity categories, with specific scores for LOC (Location), ORG (Organization), PER (Person), and MISC (Miscellaneous) entities. For instance, precision scores were 86.93% for LOC and 88.24% for ORG, while recall rates were 70.73% for LOC and 81.71% for ORG, demonstrating the model’s effectiveness in accurately identifying and classifying named entities across different categories."
      ],
      "metadata": {
        "id": "PRdTwPFJyDWC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dVt102qy5LSs"
      },
      "outputs": [],
      "source": [
        "# make prediction on the test set and save to submission.txt\n",
        "preds = predict(model, test_dataloader, device)\n",
        "with open(\"submission.txt\", \"w\") as f:\n",
        "    for tags in preds:\n",
        "        f.write(\" \".join(tags) + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}